<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.52" />
  <meta name="author" content="Sylvain Le Corff">

  
  
  
  
    
      
    
  
  <meta name="description" content="Publications about working group sessions at TSP">

  
  <link rel="alternate" hreflang="en-us" href="/project/datascienceswg/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-131234483-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Sylvain Le Corff">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Sylvain Le Corff">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/project/datascienceswg/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Sylvain Le Corff">
  <meta property="og:url" content="/project/datascienceswg/">
  <meta property="og:title" content="Data sciences working groups | Sylvain Le Corff">
  <meta property="og:description" content="Publications about working group sessions at TSP">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2016-04-27T00:00:00&#43;02:00">
  
  <meta property="article:modified_time" content="2016-04-27T00:00:00&#43;02:00">
  

  

  

  <title>Data sciences working groups | Sylvain Le Corff</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Sylvain Le Corff</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Research activities &amp; duties</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article article-project" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">

    <div class="pub-title">
      <h1 itemprop="name">Data sciences working groups</h1>
      <span class="pub-authors" itemprop="author">&nbsp;</span>
      <span class="pull-right">
        

      </span>
    </div>

    

    <div class="article-style" itemprop="articleBody">
      <p style="font-size:20px">Session 1: From machine learning basics to Feed Forward Neural Networks</p>

<p>Typical machine learning problems can be decomposed into two different classes.</p>

<p>(<strong>Classification</strong>).
The problem is to learn whether an individual from a given state space $\mathbb{R}^p$ belongs to some class. The focus is usually set on learning with a known number $M$ of classes so that an individual is associated with a label in ${1,\ldots,M}$. The statistical model is then given by $(X,Y)\in\mathbb{R}^p\times {1,\ldots,M}$ and the objective is to define a function $f: \mathbb{R}^p \to {1,\ldots,M}$, called classifier, such that $f(X)$ is the best prediction of $Y$ in a given context.</p>

<p>(<strong>Regression</strong>).
The observation associated with $X$ is assumed to be given by
$$
Y = f(X) + \varepsilon\,,
$$
where $\varepsilon$ is a centered noise independent of $X$. The statistical model is then given by $(X,Y)\in\mathbb{R}^p\times \mathbb{R}^m$ and the objective is to define the  best estimator of $f$ in a given context.</p>

<p>An element of $\mathbb{R}^p$ contains all the features the label prediction or the regression estimate has to be based on.</p>

<p><strong>Loss and risk functions</strong></p>

<p>Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space. Assume that $(X,Y)$ is a couple of random variables defined on  $(\Omega,\mathcal{F},\mathbb{P})$ and taking values in $\mathbb{R}^p\times{1,\ldots,M}$ or $\mathbb{R}^p\times \mathbb{R}^m$ where $\mathbb{R}^p$ is a given state space. In the case of nonparametric models, it is not assumed that the joint law of $(X,Y)$ belongs to any parametric or semiparametric family of models. The best prediction is defined as
$$
h_{*} \in \underset{h\in\mathcal{H}}{\mathrm{argmin}}\;\mathcal{R}(h) \quad \mathrm{where} \quad \mathcal{R}(h)= \mathbb{E}[\ell(Y,h(X))]\,,
$$
and $\ell$ is a loss function measuring the goodness of the prediction of $Y$ by $h(X)$ and $\mathcal{H}$ is a chosen set of possible candidates.  Some widespread choices of loss function are:</p>

<p>(<strong>Classification</strong>):$\quad\ell(Y,h(X)) = \left|Y-h(X)\right|^2$.</p>

<p>(<strong>Regression</strong>): $\quad\ell(Y,h(X)) = 1_{Y\neq h(X)}$.</p>

<p>In most cases, the  risk $\mathcal{R}$ cannot be computed nor minimized, it is instead estimated by the empirical classification risk defined as
$$
\mathcal{R}_n(h) = \frac{1}{n}\sum_{i=1}^n \ell(Y_i,h(X_i))\,,
$$
where $(X_i,Y_i)_{1\leqslant i\leqslant n}$ are independent observations with the same distribution as $(X,Y)$. The classification problem then boilds down to solving
$$
\widehat h_{n} \in \underset{h\in\mathcal{H}}{\mathrm{argmin}}\;\mathcal{R}_n(h) \,.
$$
In this context, several practical and theoretical challenges arise from the minimization of the empirical classification risk.</p>

<p><strong>Gentle start - classification with a mixture of two Gaussian distributions</strong></p>

<p>In this first example, consider a parametric model, that is, the joint distribution of $(X,Y)$ is assumed to belong to a family of distributions parametrized by a vector $\theta$ with real components. For $k\in{-1,1}$, write $\pi_k = \mathbb{P}(Y = k)$. Assume that conditionally on the event $\{Y = k\}$, $X$ has a Gaussian distribution with mean $\mu_k \in\mathbb{R}^p$ and covariance matrix $\Sigma\in \mathbb{R}^{p\times p}$.
The probability density function of $X$ given that ${Y=k}$ is
$$
g_{k}:x\mapsto \sqrt{\mathrm{det}\left(2\pi\Sigma\right)}\mathrm{exp}\left\{-\frac{1}{2}\left(x-\mu_k\right)&rsquo;\Sigma^{-1}\left(x-\mu_k\right)\right\}\,.
$$
In this case, the parameter $\theta=(\pi_1, \mu_1,\mu_{-1}, \Sigma)$ belongs to the set $\Theta= [0,1] \times \mathbb{R}^d \times \mathbb{R}^d \times \mathbb{R}^{d \times d}$. The parameter $\pi_{-1}$ is not part of the components of $\theta$ since $\pi_{-1}=1-\pi_{1}$.</p>

<p>The explicit computation of $\mathbb{P}(Y=1 | X)$ writes
$$
\mathbb{P}\left(Y=1\middle | X\right) = \frac{\pi_1g_1(X)}{\pi_1g_1(X) + \pi_{-1}g_{-1}(X)} = \frac{1}{1 + \frac{\pi_{-1}g_{-1}(X)}{\pi_1g_1(X)}} = \sigma\left(\log(\pi_1/\pi_{-1}) + \log(g_1(X)/g_{-1}(X)\right)\,,
$$
where $\sigma: x\mapsto (1 + \mathrm{e}^{-x})^{-1}$ is the sigmoid function. Then,
$$
\mathbb{P}\left(Y=1\middle | X\right) = \sigma\left( x&rsquo;\omega+ b\right)\,,
$$
where
$$
\omega =  \Sigma^{-1}\left(\mu_{1} - \mu_{-1}\right)\,,
b = \log(\pi_1/\pi_{-1}) +  \frac{1}{2}\left(\mu_{1}+\mu_{-1}\right)&rsquo;\Sigma^{-1}\left(\mu_{-1}-\mu_{1}\right)\,.
$$
When $\Sigma$ and $\mu_1$ and $\mu_{-1}$ are unknown, this classifier cannot be computed explicitely. We will approximate it using the observations. Assume that  $(X_i,Y_i)_{1\leqslant i\leqslant n}$ are independent observations with the same distribution as $(X,Y)$. The loglikelihood of these observations is given by
$$
\log\mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right) =\sum_{i=1}^n \log \mathbf{p}_{\theta} \left(X_{i},Y_{i}\right)\,,
$$
which yields
$$
\log\mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right) = - \frac{nd}{2} \log(2\pi)-\frac{n}{2} \log\det\Sigma + \left(\sum_{i=1}^n 1_{Y_i=1}\right)\log \pi_1 + \left(\sum_{i=1}^n 1_{Y_i=-1}\right)\log (1-\pi_{1}) -  \frac{1}{2}\sum_{i=1}^n 1_{Y_i=1}\left(X_i - \mu_{1}\right)&rsquo;\Sigma^{-1}\left(X_i - \mu_{1}\right) -  \frac{1}{2}\sum_{i=1}^n 1_{Y_i=-1}\left(X_i - \mu_{-1}\right)&rsquo;\Sigma^{-1}\left(X_i - \mu_{-1}\right)\,.
$$
Maximizing the log likelihood function to estimate $\theta$ is equivalent to minimizing the empirical cross-entropy risk function:
$$
\theta \mapsto -\frac{1}{n}\sum_{i=1}^n\sum_{k\in\{-1,1\}}1_{Y_i=k}\left(\log \pi_{k} -\frac{\log \det \Sigma}{2} - \frac{1}{2}\left(X_i - \mu_{k}\right)&rsquo;\Sigma^{-1}\left(X_i - \mu_{k}\right)\right) \,.
$$
The gradient of $\log \mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right)$ with respect to $\theta$ is therefore given by
$$
\frac{\partial \log \mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \pi_1} = \left(\sum_{i=1}^n1_{Y_i=1}\right)\frac{1}{\pi_1} - \left(\sum_{i=1}^n 1_{Y_i=-1}\right)\frac{1}{1-\pi_{1}}\,,
$$
$$
\frac{\partial \log \mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \mu_1} = \sum_{i=1}^n 1_{Y_i=1}\left(2\Sigma^{-1}X_i - 2\Sigma^{-1}\mu_{1}\right)\,,
$$
$$
\frac{\partial \log \mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \mu_{-1}} = \sum_{i=1}^n 1_{Y_i=-1}\left(2\Sigma^{-1}X_i - 2\Sigma^{-1}\mu_{-1}\right)\,,
$$
$$
\frac{\partial \log \mathbf{p}_{\theta}\left(X_{1:n},Y_{1:n}\right)}{\partial \Sigma^{-1}} = \frac{n}{2}\Sigma -  \frac{1}{2}\sum_{i=1}^n 1_{Y_i=1}\left(X_i - \mu_{1}\right)\left(X_i - \mu_{1}\right)&rsquo; -  \frac{1}{2}\sum_{i=1}^n 1_{Y_i=-1}\left(X_i - \mu_{-1}\right)\left(X_i - \mu_{-1}\right)&rsquo;\,.
$$
The maximum likelihood estimator (i.e. the cross-entropy based estimator) is defined as the only parameter such that all these equations are set to $0$. For $k\in\{-1,1\}$,  it is given by
$$
\widehat \pi_k^n = \frac{1}{n}\sum_{i=1}^n1_{Y_i=k}\,,
$$
$$
\widehat \mu_k^n = \frac{1}{\sum_{i=1}^n1_{Y_i=k}}\sum_{i=1}^n 1_{Y_i=k}\,X_i\,,
$$
$$
\widehat\Sigma^n = \frac{1}{n}\sum_{i=1}^n \left(X_i - \widehat \mu_{Y_i}^n\right)\left(X_i - \widehat \mu_{Y_i}^n\right)&rsquo;\,.
$$</p>

<p><strong>Relaxing the Gaussian assumption - Logistic regression</strong></p>

<p>In some situations, it may be too restrictive to assume that the joint distribution of $(X,Y)$ belongs to a parametric family. One of the most widespread model is the logistic regression which is defined by
$$
\mathbb{P}\left(Y=1\middle |X\right) = \sigma(X&rsquo;\omega + b)\,,
$$
where $b\in\mathbb{R}$, $\omega\in\mathbb{R}^d$ and for all $x\in\mathbb{R}^d$.
The parameter $\theta$ is thus $\theta=(b,\omega) \in \mathbb{R} \times \mathbb{R}^p$.</p>

<p>When $b$ and $\omega$ are unknown, this quantity cannot be computed explicitely and is approximated using the observations. Assume that  $(X_i,Y_i)_{1\leqslant i\leqslant n}$ are independent observations with the same distribution as $(X,Y)$. The conditional  likelihood of the observations $Y_{1:n}$  given $X_{1:n}$ is:
$$
\mathbf{p}_{\theta}\left(Y_{1:n}\middle |X_{1:n}\right) =\prod_{i=1}^n \mathbf{p}_{\theta}\left(Y_{i} \middle |X_{i}\right)= \prod_{i=1}^n \left(\sigma(X_i)\right)^{(1+Y_i)/2}\left(1-\sigma(X_i)\right)^{(1-Y_i)/2}\,,
$$
which yields
$$
\mathbf{p}_{\theta}\left(Y_{1:n}\middle |X_{1:n}\right) =  \prod_{i=1}^n \left(\frac{\mathrm{e}^{b + \langle \omega;X_i\rangle}}{1+\mathrm{e}^{b + \langle \omega;X_i\rangle}}\right)^{(1+Y_i)/2}\left( \frac{1}{1+\mathrm{e}^{b + \langle \omega;X_i\rangle}}\right)^{(1-Y_i)/2}\,.
$$
The associated conditional loglikelihood is therefore
$$
\log \mathbf{p}_{\theta}\left(Y_{1:n}\middle |X_{1:n}\right) =\sum_{i=1}^n \left\{\frac{1+Y_i}{2}\log\left(\frac{\mathrm{e}^{b + \langle \omega;X_i\rangle}}{1+\mathrm{e}^{b + \langle \omega;X_i\rangle}}\right) + \frac{1-Y_i}{2}\log\left(\frac{1}{1+\mathrm{e}^{b + \langle \omega;X_i\rangle}}\right) \right\}\,,
$$
i.e.
$$
\log \mathbf{p}_{\theta}\left(Y_{1:n}\middle |X_{1:n}\right)  = \sum_{i=1}^n \left\{ \frac{1+Y_i}{2}\left(b + \langle \omega;X_i\rangle\right) - \log\left(1+\mathrm{e}^{b+ \langle \omega;X_i\rangle}\right)\right\}\,.
$$
Note again that maximizing this loglikelihood is equivalent to minimizing the empirical cross-entropy. It cannot be done explictly yet numerous numerical optimization methods are available to maximize $(\omega,b)\mapsto \log \mathbf{p}_{\theta}\left(Y_{1:n}\middle |X_{1:n}\right)$ (<strong>see next session on a up-to-date overview of gradient based algorithms</strong>).</p>

<p><strong>The multilayer perceptron - Feed Forward Neural Networks</strong></p>

<p>The first  mathematical model for a neuron was the Threshold Logic Unit <a href="http://www.cse.chalmers.se/~coquand/AUTOMATA/mcp.pdf">(McCulloch and Pitts, 1943)</a>, with Boolean inputs and outputs. The response associated with an input $x\in\{0,1\}^d$ is defined as $f: x\mapsto 1_{\omega\sum_{j=1}^dx_j + b \geqslant 0}$. This construction allows to build any boolean function from elementary units. This elementary model can be extended to the <strong>Perceptron</strong> with real valued inputs <a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf">(Rosenblatt, 1957)</a> by writing  $f: x\mapsto 1_{\sum_{j=1}^d\omega_jx_j + b \geqslant 0}$. In this case, the nonlinear activation function is $\sigma: x \mapsto 1_{x\geqslant 0}$ and the ouput defined as:
$$
f:x \mapsto \sigma(\omega&rsquo;x + b)\,.
$$
Linear discriminant analysis and logistic regression are other instances with the sigmoid activation function. The perceptron weakens the modeling assumptions of LDA or logistic regression and composed in parallel  $q$ of these perceptron units to produce the output. Then, $x = z_0\in\mathbb{R}^p$, $b_1\in \mathbb{R}^q$, $\omega_1\in\mathbb{R}^{p\times q}$ and
$$
z_1 = \sigma(\omega_1&rsquo;x + b)\,,
$$
with $\sigma$ the elementwise activation function. The <strong>multi-layer perceptron</strong>, also known as the fully connected feedforward network, connects these units in series. For a given number $L$ of layers,
$$
z_1 = \sigma(\omega_1&rsquo;x + b_1) \,, \quad z_2 = \sigma(\omega_2&rsquo;z_1 + b_2)\,,\quad\ldots\,,\quad  z_L = \sigma(\omega_L&rsquo;z_{L-1} + b_L)\,.
$$
As there is no modelling assumptions anymore, virtually any activation function may be used. The relu activation
 $x \mapsto \mathrm{max}(0,x)$ and its extensions are the default recommendation in modern implementations  <a href="http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf">(Jarrettet al., 2009)</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6419&rep=rep1&type=pdf">(Nair and Hinton, 2010)</a>, <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">(Glorot et al., 2011)</a>, etc. One of the major motivation arises from the gradient based parameter optimization which is numerically more stable with relu, (<strong>see next session on a up-to-date overview of gradient based algorithms</strong>). Assume that the network contains $L$ layers, then the output layer is of the form:
$$
z_L = \sigma(\omega_L&rsquo;z_{L-1} + b_L)\,.
$$
The choice of this last activation function greatly relies on the task the network is assumed to perform.</p>

<p>(<strong>Biclass classification</strong>). The output $z_L$ is the estimate of the probability that the class is $1$ given the input $x$. The common choice in this case is the sigmoid function, one  of the main reason is due to the gradient descent algorithm used to optimize the parameters, cf. next sessions.</p>

<p>(<strong>Multiclass classification</strong>). The output $z_L$ is the estimate of the probability that the class is  $k$ for all $1\leqslant k\leqslant M$, given the input $x$. The common choice in this case is the softmax function: for all $1\leqslant i\leqslant r$,
$$
\sigma(z)_i = \frac{\mathrm{e}^{z_i}}{\sum_{j=1}^r\mathrm{e}^{z_j}}\,.
$$</p>

<p><strong>Universal approximation properties</strong></p>

<p>The universal approximation theorem sets a theoretical guarentee that feedforward networks with hidden layers provide a universal approximation framework. The first result of <a href="http://cognitivemedium.com/magic_paper/assets/Hornik.pdf">(Horniket al., 1989)</a> and <a href="https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf">(Cybenko, 1989)</a> states that a feedforward network with a linear output layer and at least one hidden layer  can approximate any Borel measurable function from one finite-dimensional space to another, provided that the network is given enough hidden units. For a given activation function $\sigma:\mathbb{R}\to\mathbb{R}$, the set of neural networks with one hidden layer and a linear output associated with $\sigma$ is given by:
$$
\mathcal{N}_{\sigma} = \left\{f:\mathbb{R}^p\to \mathbb{R}\,;\, \exists\, q\geqslant 1\,, (c_1,\ldots,c_q, b_1,\ldots,b_q)\in\mathbb{R}^{2q}\,,(\omega_1,\ldots,\omega_q)\in\mathbb{R}^{pq}\, \, f: x\mapsto \sum_{j=1}^q c_j\sigma(\langle \omega_j;x\rangle + b_j)\right\}\,.
$$</p>

<p><strong>(Horniket al., 1989), (Cybenko, 1989)</strong>.<br />
Let $\sigma:\mathbb{R}\to \mathbb{R}$ be a continuous activation function such that $\lim_{x\to \infty}\sigma(x) = 1$ and $\lim_{x\to -\infty}\sigma(x) = 0$. Then, $\mathcal{N}_{\sigma}$ is dense in $\mathcal{C}([0,1]^p,\mathbb{R})$ for the topology of the supremum norm.</p>

<p>While the original theorems were first stated in terms of units with activation functions that saturate for both very negative and very positive arguments, universal approximation theorems have also been proved for a wider class of activation functions, which includes the now commonly used rectified linear unit <a href="http://www2.math.technion.ac.il/~pinkus/papers/neural.pdf">(Leshno et al., 1993)</a>.</p>

<p><strong>(Leshno et al., 1993)</strong>.
Assume that $\sigma:\mathbb{R}\to \mathbb{R}$ is continuous and is not a polynomial activation function. Then, $\mathcal{N}_{\sigma}$ is dense in $\mathcal{C}(\mathbb{R}^p,\mathbb{R})$ for the topology of the supremum norm on compact subsets.</p>

<p>According to the universal approximation theorem, there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be. In <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">(Barron, 1993)</a>, the authors provides some bounds on the size of a single-layer network needed to approximate a broad class of functions. Unfortunately, in the worst case, an exponential number of hidden units (possibly with one hidden unit corresponding to each input configuration that needs to be distinguished) may be required.</p>

<p><strong>(Barron, 1993)</strong>.
Let $r&gt;0$ and $\mu$ be a probability distribution on the closed ball centered at 0 and with radius $r$ denoted by $\mathsf{B}_r$. Let $\sigma:\mathbb{R}\to \mathbb{R}$ be a bounded and measurable activation function such that $\lim_{x\to \infty}\sigma(x) = 1$ and $\lim_{x\to -\infty}\sigma(x) = 0$. Assume that $f:\mathbb{R}^p\to \mathbb{R}$ is such that its Fourier transform $\widetilde f$ satisties $\int_{\mathbb{R}^p} |\omega||\widetilde{f}(w)|\mathrm{d} \omega&lt;\infty$. Then, for all $q\geqslant 1$, there exist a Feedforward neural network model with one layer of $q$
sigmoidal units:
$$
f_q: x\mapsto \sum_{j=1}^q c_j\sigma(\langle \omega_j;x\rangle + b_j) + c_0
$$
and $C&gt;0$ such that
$$
\int_{\mathsf{B}_r}\left(f(x) - f_q(x)\right)^2\mu(\mathrm{d} x) \leqslant \frac{C}{q}\,.
$$</p>

<p style="font-size:20px">Session 2: Optimization for machine/deep learning</p>

<p>Deep learning optimization problems are of the form
$$
\argmin_{w \in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n \ell(y_i, \langle w, x_i \rangle) + \lambda g(w)\,,
$$
where $\ell$ is a given loss function and $g$ is a penalization function such as $g: w \mapsto |w|_2^2$ or $g: w \mapsto |w|_1^2$.</p>

<p>Let $f: \mathbb{R}^d \to \mathbb{R}$ be a differentiable function. If $x^{\star}$ is a local extremum then $\nabla f(x^{\star}) = 0$, where
$\nabla f(x^{\star})$ is the gradient of the function $f$ at point $x^{\star}$:
$$
\nabla f(x) = \begin{pmatrix} \frac{\partial f}{\partial x_1} \ \vdots \ \frac{\partial f}{\partial x_d}\end{pmatrix}\,.
$$
For instance, if $f: x \mapsto \langle a,x\rangle$, then  $\nabla f(x) = a$ and  if $f: x \mapsto x^T A x$ then $\nabla f(x) = (A + A^T) x$.</p>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="/tags/data-sciences/">Data sciences</a>
  
  <a class="label label-default" href="/tags/ai/">AI</a>
  
  <a class="label label-default" href="/tags/machine-learning/">Machine learning</a>
  
  <a class="label label-default" href="/tags/working-group/">Working group</a>
  
  <a class="label label-default" href="/tags/maths/">Maths</a>
  
</div>




    
    
    

    
      
      
      
      

      
      
      
      
    

  </div>
</article>



<footer class="site-footer">
  <div class="container">

    
    <p class="powered-by">
      <a href="/privacy/">Privacy Policy</a>
    </p>
    

    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous"></script>
    
    

    
    

  </body>
</html>

