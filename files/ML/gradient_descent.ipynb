{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkorange> Gradient descent algorithms</font>\n",
    "### <font color=darkorange> Application to logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "np.set_printoptions(precision=2) \n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import check_grad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred>Logistic regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightharpoondown$ The objective is to predict the  label $Y\\in\\{0,1\\}$ based on $X\\in\\mathbb{R}^d$.\n",
    "\n",
    "$\\rightharpoondown$ Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "where $w \\in \\mathbb{R}^d$ is a vector of model weights and $b \\in \\mathbb{R}$ is the intercept, and where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\sigma: z \\mapsto \\frac{1}{1 + e^{-z}}\\,.\n",
    "$$\n",
    "\n",
    "$\\rightharpoondown$ The sigmoid function is a \\alert{model choice to map $\\mathbb{R}$ into $(0,1)$}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    expx = np.exp(x)\n",
    "    z = expx / (1. + expx)\n",
    "    return z\n",
    "\n",
    "def sample_logistic(w0, n_samples=1000, corr=0.5):\n",
    "    n_features = w0.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(X.dot(w0))\n",
    "    y = np.random.binomial(1, p, size=n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(x,z,linestyle = \"dashed\",color=\"blue\", label= \"Sigmoid function\")\n",
    "plt.plot(x,.5*np.ones(np.size(x)),linestyle = \"dashed\",color=\"red\", label=\"Threshold 1/2\")\n",
    "plt.ylabel('Probability of  the event {Y=1}', fontsize=12)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "The graph above illustrates that the Bayes classification rule in this case is\n",
    "$f^*(X) = 1$ if and only if $\\langle w,X \\rangle + b>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 2\n",
    "\n",
    "w0 = multivariate_normal([-2,1], np.eye(2))\n",
    "\n",
    "X, y = sample_logistic(w0, n_samples=n_samples, corr = 0.3)\n",
    "\n",
    "simulated_data = pd.DataFrame(columns = [\"x\",\"y\",\"Label\"])\n",
    "simulated_data[\"x1\"] = X[:,0]\n",
    "simulated_data[\"x2\"] = X[:,1]\n",
    "simulated_data[\"Label\"] = y\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X,w):\n",
    "    z = sigmoid(X.dot(w))\n",
    "    return z\n",
    "    \n",
    "xlim = [np.min(X[:,0]), np.max(X[:,0])]\n",
    "ylim = [np.min(X[:,1]), np.max(X[:,1])]\n",
    "xplot = np.linspace(xlim[0], xlim[1], 30)\n",
    "yplot = np.linspace(ylim[0], ylim[1], 30)\n",
    "\n",
    "Yplot, Xplot = np.meshgrid(yplot, xplot)\n",
    "xy = np.vstack([Xplot.ravel(), Yplot.ravel()]).T\n",
    "P = decision_function(xy,w0).reshape(Xplot.shape)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");\n",
    "\n",
    "# plot decision boundary and margins\n",
    "plt.contour(Xplot, Yplot, P, colors = 'k', levels = [0.5], alpha = 0.8, linestyles = ['--']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Logistic regression: losses and gradients </font>\n",
    "\n",
    "The aim of this section is to detail how to solve the following optimization problem\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\\,,\n",
    "$$\n",
    "where $d$ is the number of features.\n",
    "\n",
    "$$\n",
    "f: w \\mapsto \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\{-y_ix_i'w + \\log(1 + \\exp(x_i' w))\\} + \\frac{\\lambda}{2} \\|w\\|_2^2\\,,\n",
    "$$\n",
    "where $n$ is the sample size, and where $y_i \\in \\{ 0, 1 \\}$ for all $1\\leqslant i\\leqslant n$.\n",
    " \n",
    "A basic gradient descent algorithm requires to compute the functions $f$ and $\\nabla f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Write the likelihood of the logistic regression model, its gradient and partial derivatives (blackboard)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, X, y, lmbd):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lmbd = lmbd\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        # Computes f(w)\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        for k in range(n_samples):\n",
    "            res += np.log(1+np.exp(X[k].dot(w))) - y[k]*X[k].dot(w)\n",
    "        res = res/n_samples\n",
    "        res += lmbd*0.5*(norm(w)**2)\n",
    "        return res\n",
    "    \n",
    "    def grad(self, w):\n",
    "        # Computes the gradient of f at w\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        for k in range(n_samples):\n",
    "            res += -y[k]*X[k] + np.exp(X[k].dot(w))/(1+np.exp(X[k].dot(w)))*X[k]\n",
    "        res = res/n_samples\n",
    "        res += lmbd*w\n",
    "        return res  \n",
    "    \n",
    "    def grad_fi(self, i, w):\n",
    "        # Computes the gradient of f_i at w\n",
    "        x_i = self.X[i]\n",
    "        return -y[i]*x_i + np.exp(x_i.dot(w))/(1+np.exp(x_i.dot(w)))*x_i + self.lmbd * w\n",
    "\n",
    "    def grad_coordinate(self, j, w):\n",
    "        # Computes the partial derivative of f with respect to the j-th coordinate \n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        for k in range(n_samples):\n",
    "            res += -y[k]*X[k,j] + np.exp(X[k].dot(w))/(1+np.exp(X[k].dot(w)))*X[k,j]\n",
    "        res = res/n_samples\n",
    "        res += lmbd*w[j]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numerically the gradient using the function checkgrad from scipy.optimize\n",
    "# Use the function simu_logreg to simulate data according to the logistic regression model\n",
    "n_features = 10\n",
    "w_true     = np.random.randn(n_features)\n",
    "X, y       = sample_logistic(w_true, n_samples, corr=0.9)\n",
    "model      = LogisticRegression(X, y, 1e-3)\n",
    "# check_grad assesses the correctness of a gradient by comparing it to a finite-difference approximation\n",
    "check_grad(model.loss, model.grad, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Gradient descent </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of machine/deep learning applications, the function to be minimized is of the form:\n",
    "$$\n",
    "f:w\\mapsto  \\frac{1}{n}\\sum_{i=1}^n \\ell(Y_i, \\langle w; X_i \\rangle) + \\lambda g(w) = \\frac{1}{n}\\sum_{i=1}^n f_i(w)\\,.\n",
    "$$\n",
    "The most simple method  is based on full gradients, since at each iteration  it requires to compute\n",
    "$$\n",
    "\\nabla f(w) = \\frac 1n \\sum_{i=1}^n \\nabla  f_i(w)\\,,\n",
    "$$\n",
    "which depends on the whole dataset. When processing very large datasets ($n$ is large), this approach has a highly prohibitive computational cost  for a  unique step towards the minimum. \n",
    "For all $k\\geqslant 1$, set\n",
    "$$\n",
    "w^{(k)} = w^{(k-1)} - \\eta_k \\nabla f_{}(w^{(k-1)})\\,.\n",
    "$$\n",
    "Each iteration has complexity $O(nd)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter,step):\n",
    "    loss_val = np.zeros(n_iter+1)\n",
    "    w        = w0.copy()\n",
    "    w_new    = w0.copy()\n",
    "    for k in range(n_iter + 1):\n",
    "        w_new[:]    = w-step*model.grad(w)\n",
    "        w[:]        = w_new\n",
    "        loss_val[k] = model.loss(w)\n",
    "    return w, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.tight_layout()\n",
    "\n",
    "step_gd = [1e-2,1e-1,5e-1,1,2]\n",
    "w0 = np.random.randn(n_features)\n",
    "n_iter_max = 500\n",
    "for stp in step_gd:\n",
    "    w, gd_loss = gd(model, w0,n_iter_max, stp)\n",
    "    plt.plot(gd_loss, '-', label = 'Step size %f'%stp)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Stochastic gradient descent </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model, w0, n_iter, step, alpha):  \n",
    "    loss_val  = np.zeros(n_iter)\n",
    "    w = w0.copy()\n",
    "    n_samples = model.n_samples\n",
    "    for idx in range(n_iter):\n",
    "        i = np.random.randint(0, model.n_samples)\n",
    "        w = w - step * model.grad_fi(i,w) / ((idx+1)**alpha)\n",
    "        loss_val[idx] = model.loss(w)\n",
    "    return w, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.title('Stochastic gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.tight_layout()\n",
    "\n",
    "step_gd = [1e-2,1e-1,5e-1,1,2]\n",
    "stp     = 0.5\n",
    "alpha   = [0.5,0.6,0.7,0.8,0.9]\n",
    "w0 = np.random.randn(n_features)\n",
    "n_iter_max = 700\n",
    "for al in alpha:\n",
    "    w, sgd_loss = sgd(model, w0,n_iter_max, stp, al)\n",
    "    plt.plot(sgd_loss, '-', label = 'Rate alpha %f'%al)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Coordinate gradient descent </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgd(model, w0, n_iter, step):\n",
    "    w = w0.copy()\n",
    "    loss_val  = np.zeros(n_iter+1)\n",
    "    n_features = model.n_features\n",
    "    for k in range(n_iter + 1):\n",
    "        for j in range(n_features):\n",
    "            w[j] = w[j]-step*model.grad_coordinate(j,w)\n",
    "        loss_val[k] = model.loss(w)\n",
    "    return w, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cgd = 1e-1\n",
    "n_iter_max = 500\n",
    "\n",
    "w, cgd_loss = cgd(model, w0, n_iter_max, step_cgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.plot(cgd_loss, '-')\n",
    "plt.title('Coordinate gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Nesterov accelerated gradient descent </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesgd(model, w0, n_iter, step, beta):\n",
    "    loss_val = np.zeros(n_iter+1)\n",
    "    w = w0.copy()\n",
    "    v = w0.copy()\n",
    "    v_new = w0.copy()\n",
    "    for k in range(n_iter + 1):\n",
    "        v_new[:] = w-step*model.grad(w)\n",
    "        w[:] = v_new + beta*(v_new-v)\n",
    "        v = v_new.copy()\n",
    "        loss_val[k] = model.loss(w)\n",
    "    return w, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_nesgd = 1e-1\n",
    "beta = 0.95\n",
    "n_iter_max = 500\n",
    "\n",
    "w, nes_loss = nesgd(model, w0, n_iter_max, step_nesgd, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.plot(nes_loss, '-')\n",
    "plt.title('Nesterov gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> All together </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.plot(gd_loss, '-', label = 'Full gradient descent')\n",
    "plt.plot(sgd_loss, '-', label = 'Stochastic gradient descent')\n",
    "plt.plot(cgd_loss, '-', label = 'Coordinate gradient descent')\n",
    "plt.plot(nes_loss, '-', label = 'Nesterov accelerated gradient descent')\n",
    "plt.title('Gradient descent algorithms')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Performance analysis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare the minimizers you obtain using the different algorithms, with a large and a small number of iterations and several step sizes.\n",
    "\n",
    "2. Analyze the influence of the correlation of the features on the performance of the optimization algorithms. \n",
    "\n",
    "3. Analyze the influence of $\\lambda$ on the performance of the optimization algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a gradient descent algorithm for a linear regression problem where the loss function is given below.\n",
    "$$\n",
    "f: w \\mapsto \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i' w)^2 + \\frac{\\lambda}{2} \\|w\\|_2^2 = \\frac{1}{2 n} \\| y - X w \\|_2^2 + \\frac{\\lambda}{2} \\|w\\|_2^2\\,,\n",
    "$$\n",
    "where $n$ is the sample size, $y = (y_1, \\ldots, y_n)'\\in\\mathbb{R}^n$ and $X$ is the matrix of features with lines containing the features vectors $x_i \\in \\mathbb R^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
