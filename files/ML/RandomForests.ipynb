{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan>  Kernel based regression and random forests </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange>  1. Independent simulated data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import multivariate_normal\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(precision=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample a dataset - linear observation model\n",
    "def sample_data_iid_linear(alpha,sigmay,n,d):\n",
    "    X       = np.random.uniform(0,2*np.pi,[n,d])\n",
    "    Y       = np.zeros(n)\n",
    "    eta     = np.random.normal(loc=0,scale=1,size = n)\n",
    "    Y[0]    = np.sum(np.cos(X[0,:])) + sigmay*eta[0]\n",
    "    for k in range(1,n):\n",
    "        Y[k]   = np.sum(alpha*X[k,:]) + sigmay*eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample a dataset - nonlinear observation model\n",
    "def sample_data_iid_signal(sigmay,n,d):\n",
    "    X       = np.random.uniform(0,2*np.pi,[n,d])\n",
    "    Y       = np.zeros(n)\n",
    "    eta     = np.random.normal(loc=0,scale=1,size = n)\n",
    "    Y[0]    = np.sum(np.cos(X[0,:])) + sigmay*eta[0]\n",
    "    for k in range(1,n):\n",
    "        Y[k]   = np.sum(np.cos(X[k,:])) + sigmay*eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n_train = 1000\n",
    "n_test = 100\n",
    "d = 2\n",
    "sigmay = 0.1 \n",
    "true_alpha =  multivariate_normal(np.zeros(d), np.eye(d))\n",
    "true_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data  \n",
    "X_train, Y_train = sample_data_iid_linear(true_alpha,sigmay,n_train,d)\n",
    "X_test, Y_test = sample_data_iid_linear(true_alpha,sigmay,n_test,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkred>  Linear regression for a simulated data set </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed that for all $1\\leqslant i \\leqslant n$, \n",
    "\n",
    "$$\n",
    "Y_i = X^T_i \\beta_{\\star} + \\varepsilon_i\\,,\n",
    "$$\n",
    "\n",
    "where the $(\\varepsilon_i)_{1\\leqslant i\\leqslant n}$ are i.i.d. random variables in $\\mathbb{R}^d$, $X_i\\in\\mathbb{R}^d$ and $\\beta_{\\star}$ is an unknown vector in $\\mathbb{R}^d$. Let $Y\\in\\mathbb{R}^d$ (resp. $\\varepsilon\\in\\mathbb{R}^d$)  be the random vector such that  for all $1\\leqslant i \\leqslant n$, the $i$-th component of $Y$ (resp. $\\varepsilon$) is $Y_i$ (resp. $\\varepsilon_i$) and $X\\in\\mathbb{R}^{n\\times d}$ the matrix with line $i$ equal to $X^T_i$. The model is then written\n",
    "\n",
    "$$\n",
    "Y = X \\beta_{\\star} + \\varepsilon\\,.\n",
    "$$\n",
    "\n",
    "In this section, it is assumed that $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathbb{E}[\\varepsilon \\varepsilon^T] = \\sigma_{\\star}^2 I_n$. The penalized least squares estimate of $\\beta_{\\star}$ is defined as a solution to\n",
    "\n",
    "$$\n",
    "\\widehat \\beta_n\\in  \\mathrm{argmin}_{\\beta\\in\\mathbb{R}^d}\\,\\left( \\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\\right)\\,.\n",
    "$$\n",
    "\n",
    "where $\\lambda>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Explain why the loss function is penalized (blackboard) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $X^TX + \\lambda I_n$ is definite positive for all $\\lambda>0$ as for all $u\\in\\mathbb{R}^d$,\n",
    "\n",
    "$$\n",
    "u^T(X^TX + \\lambda I_n)u = \\|Xu\\|_2^2 + \\lambda \\|u\\|_2^2\\,,\n",
    "$$\n",
    "\n",
    "which is positive for all $u\\neq 0$. \n",
    "\n",
    "Therefore,  the matrix $X^TX + \\lambda I_n$ is invertible for all $\\lambda>0$. Using that for all $\\beta>0$,\n",
    "\n",
    "$$\n",
    "\\nabla \\left(\\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\\right) = 2X^TX\\beta - 2X^TY +  2\\lambda\\beta = 2\\left\\{\\left(X^TX + \\lambda I_d\\right)\\beta -X^TY\\right\\}\\,.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\widehat \\beta_n = \\left(X^TX + \\lambda I_d\\right)^{-1}X^TY\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Prove that the bias is given by (blackboard)\n",
    "$$\n",
    "\\mathbb{E}[\\widehat \\beta_n] - \\beta_* = - \\lambda(X^TX + \\lambda I_d)^{-1}\\beta_*\\,.\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note first that $\\mathbb{E}[Y] = X\\beta_*$ which yields\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\widehat \\beta_n] = \\left(X^TX + \\lambda I_d\\right)^{-1}X^TX\\beta_*\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\widehat \\beta_n] - \\beta_* = \\left(X^TX + \\lambda I_d\\right)^{-1}X^TX\\beta_* - \\beta_* = \\left(X^TX + \\lambda I_d\\right)^{-1}\\left(X^TX - \\left(X^TX + \\lambda I_d\\right)\\right)\\beta_* = - \\lambda(X^TX + \\lambda I_d)^{-1}\\beta_*\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Prove that the variance is given by (blackboard)\n",
    "$$\n",
    "\\mathbb{V}[\\widehat \\beta_n] = \\sigma_\\star^2(X^TX + \\lambda I_d)^{-2}X^TX\\,.\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition of $\\widehat\\beta_n$,\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[\\widehat \\beta_n] = \\left(X^TX + \\lambda I_d\\right)^{-1}X^T\\mathbb{V}[Y]X\\left(X^TX + \\lambda I_d\\right)^{-1} = \\sigma_*^2\\left(X^TX + \\lambda I_d\\right)^{-1}X^TX\\left(X^TX + \\lambda I_d\\right)^{-1}\\,,\n",
    "$$\n",
    "\n",
    "since $\\mathbb{V}[Y] = \\sigma_*^2 I_n$. Let $\\lambda_1,\\ldots,\\lambda_d\\geqslant 0$ be the eignevalues of $X^TX$. Then, there exits an orthogonal matrix $P$, such that\n",
    "\n",
    "$$\n",
    "X^TX = P^{-1}\\Delta P \\quad\\mathrm{and}\\quad X^TX + \\lambda I_d = P^{-1}\\tilde\\Delta P\\,,\n",
    "$$\n",
    "\n",
    "where $\\Delta = \\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_d)$ and $\\tilde\\Delta = \\mathrm{diag}(\\lambda_1 + \\lambda,\\ldots,\\lambda_d+\\lambda)$.\n",
    "\n",
    "$$\n",
    "\\left(X^TX + \\lambda I_d\\right)^{-1}X^TX\\left(X^TX + \\lambda I_d\\right)^{-1} = P^{-1}\\tilde\\Delta^{-1} PP^{-1}\\Delta PP^{-1}\\tilde\\Delta^{-1} P = P^{-1}\\tilde\\Delta^{-2}PP^{-1}\\Delta P = \\sigma_\\star^2(X^TX + \\lambda I_d)^{-2}X^TX\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ridge_model = Ridge()\n",
    "parameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1, 2, 3, 4, 5]}\n",
    "ridge_regressor = GridSearchCV(linear_ridge_model, parameters,scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_regressor.fit(X_train, Y_train)\n",
    "ridge_regressor.best_params_\n",
    "ridge_regressor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "Y_pred_linear = ridge_regressor.predict(X_test)\n",
    "plt.plot(Y_test,linestyle = \"dashed\",color=\"blue\", label= \"True observations\")\n",
    "plt.plot(Y_pred_linear,linestyle = \"dashed\",color=\"red\", label=\"Predictions\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Individual indices', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data  \n",
    "X_train, Y_train = sample_data_iid_signal(sigmay,n_train,d)\n",
    "X_test, Y_test = sample_data_iid_signal(sigmay,n_test,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ridge_model = Ridge()\n",
    "parameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1, 2, 3, 4, 5]}\n",
    "ridge_regressor = GridSearchCV(linear_ridge_model, parameters,scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_regressor.fit(X_train, Y_train)\n",
    "ridge_regressor.best_params_\n",
    "ridge_regressor.best_score_\n",
    "ridge_regressor.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "Y_pred_linear = ridge_regressor.predict(X_test)\n",
    "plt.plot(Y_test,linestyle = \"dashed\",color=\"blue\", label= \"True observations\")\n",
    "plt.plot(Y_pred_linear,linestyle = \"dashed\",color=\"red\", label=\"Predictions\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Individual indices', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=darkred>  Kernel regression for a simulated data set </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ is said to be a positive semi-definite kernel if and only if it is symmetric and if for all $n\\geqslant 1$, $(x_1,\\ldots,x_n)\\in(\\mathbb{R}^d)^n$ and all $(a_1,\\ldots,a_n)\\in\\mathbb{R}^n$,\n",
    "$$\n",
    "\\sum_{1\\leqslant i,j\\leqslant n}a_ia_jk(x_i,x_j) \\geqslant 0\\,.\n",
    "$$\n",
    "\n",
    "The following functions, defined on $\\mathbb{R}^d\\times\\mathbb{R}^d$, are positive semi-definite kernels:\n",
    "\n",
    "$$\n",
    "k:(x,y)\\mapsto x^Ty \\quad\\mathrm{and}\\quad k:(x,y)\\mapsto \\mathrm{exp}\\left(-\\|x-y\\|^2/(2\\sigma^2\\right)\\,,\\; \\sigma>0\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\mathcal{F}$ be a Hilbert space of functions $f:\\mathbb{R}^d\\to\\mathbb{R}$. A symmetric function $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ is said to be a reproducing kernel of $\\mathcal{F}$ if and only if:\n",
    "\n",
    "1. for all $x\\in\\mathbb{R}^d$, $k(x,\\cdot)\\in\\mathcal{F}$ ; \n",
    "\n",
    "2. for all $x\\in\\mathbb{R}^d$ and all $f\\in\\mathcal{F}$, $\\langle f; k(x,\\cdot)\\rangle_\\mathcal{F} = f(x)$ . \n",
    "\n",
    "The space $\\mathcal{F}$ is said to be a reproducing kernel Hilbert space with kernel $k$.\n",
    "\n",
    "\n",
    "\n",
    "Let $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ be a positive definite kernel and $\\mathcal{F}$ the RKHS with kernel $k$. Then (see previous session), \n",
    "\n",
    "$$\n",
    "\\widehat f^n_{\\mathcal{F}} \\in \\underset{f\\in\\mathcal{F}}{\\mathrm{min}}\\;\\frac{1}{n}\\sum_{i=1}^n (Y_i - f(X_i))^2 + \\lambda\\|f\\|_\\mathcal{F}^2\\,,\n",
    "$$\n",
    "\n",
    "where $\\|f\\|^2_\\mathcal{F} = \\langle f\\,;\\, f\\rangle_\\mathcal{F}$, is given by $\\widehat f^n_{\\mathcal{F}} : x \\mapsto \\sum_{i=1}^n \\widehat \\alpha_i k(X_i,x)$, where\n",
    "\n",
    "$$\n",
    "\\widehat\\alpha \\in \\underset{\\alpha \\in (\\mathbb{R}^d)^n}{\\mathrm{argmin}}\\;\\left\\{\\frac{1}{n}\\|Y - K\\alpha\\|^2_2 + \\lambda \\sum_{1\\leqslant i,j \\leqslant n}\\alpha_i \\alpha_j k(X_i,X_j) = \\frac{1}{n}\\|Y - K\\alpha\\|^2_2 + \\lambda \\alpha^TK\\alpha\\right\\}\\,,\n",
    "$$\n",
    "\n",
    "where for all $1\\leqslant i,j\\leqslant n$, $K_{i,j} = k(X_i,X_j)$.\n",
    "\n",
    "In practice, once the matrix $K$ is built, kernel ridge regression boils down to solving this optimization problem to obtain $\\widehat \\alpha$. Then, the estimated function $\\widehat f^n_{\\mathcal{F}}$ is a mixture of kernels evaluated at each data points with weights given by $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Provide the value of $\\widehat \\alpha$ when $K$ is invertible</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is enough to follow the exact same steps as in the standard linear setting\n",
    "$$\n",
    "\\nabla \\left(\\frac{1}{n}\\|Y - K\\alpha\\|^2_2 + \\lambda \\alpha^TK\\alpha\\right) = \\frac{2}{n}K^TK\\alpha - \\frac{2}{n}K^TY +  2\\lambda\\alpha = 2\\left\\{\\left(\\frac{1}{n}K^TK + \\lambda I_n\\right)\\alpha - \\frac{1}{n}K^TY\\right\\}\\,.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\widehat \\alpha = \\frac{1}{n}\\left(\\frac{1}{n}K^TK + \\lambda I_n\\right)^{-1}K^TY\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, performing kernel regression amounts to choosing a kernel. Four common options are available: ``linear``,`` polynomial``, ``RBF`` and ``laplacian``. Then, obtaining the optimal parameters (the value of $\\lambda$ and the parameters involved in the kernel function) may be done with ``GridSearchCV`` (see cell below) or ``RandomizedSearchCV`` (see random forests below). ``GridSearchCV`` performs an exhaustive search over the provided parameter values with cross-validation (based on ``cv`` number of folds) with ```scoring``` function to evaluate the predictions on the test set for each fold. In the case of ``RandomizedSearchCV``, not all parameter values are tested but only ``n_iter`` settings are sampled from specified values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_ridge_regressor = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5, \n",
    "                                      param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "                                                  \"gamma\": np.logspace(-2, 2, 5)})\n",
    "kernel_ridge_regressor.fit(X_train, Y_train)\n",
    "kernel_ridge_regressor.best_params_\n",
    "kernel_ridge_regressor.best_score_\n",
    "kernel_ridge_regressor.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "Y_pred_kernel = kernel_ridge_regressor.predict(X_test)\n",
    "plt.plot(Y_test,linestyle = \"dashed\",color=\"blue\", label= \"True observations\")\n",
    "plt.plot(Y_pred_kernel,linestyle = \"dashed\",color=\"red\", label=\"Predictions\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Individual indices', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> 2. Random forests for simulated time series</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Markov chain, given by $X_0 = x_0$ for $x_0\\in \\mathbb{R}^d$ and, for $k\\geqslant 0$,\n",
    "\n",
    "$$\n",
    "X_{k+1} = \\rho X_k + \\sigma_X\\varepsilon_k\\,\\,\\, [2\\pi]\\,\n",
    "$$ \n",
    "\n",
    "where $(\\epsilon_k)_{k\\geqslant 0}$ are i.i.d. standard Gaussian vectors in $\\mathbb{R}^d$ $\\sim {\\sf N}(0,{\\bf I}_d)$.\n",
    "\n",
    "The observation model is\n",
    "\n",
    "$$ \n",
    "Y_k = f(X_k) + \\sigma_Y\\eta_{k}\\,,\n",
    "$$\n",
    "\n",
    "where $(\\eta_k)_{k\\geqslant 0}$ are i.i.d. $\\sim N(0,1)$ and\n",
    "\n",
    "$$\n",
    "f:\\begin{cases}\n",
    "\\mathbb{R}^d\\to\\mathbb{R}\\\\\n",
    "{\\bf x} \\mapsto \\sum_{i=1}^{d}\\cos(x_i)\n",
    "\\end{cases}\\,.\n",
    "$$\n",
    "\n",
    "The objective is to estimate the function $f$ using a training data set to predict the observations associated with the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a few imports, in particular the function ``RandomForestRegressor`` of sklearn that enables to apply the random forest algorithm in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes as input the size $d$ of the vector $x_0$, the size $n$ of the dataset, the noise levels $\\sigma_X$ of the autoregressive model and $\\sigma_Y$ of the observed values and the parameter $\\rho$ in the autoregressive model.\n",
    "\n",
    "The function outputs data following the model described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample a dataset \n",
    "def sample_data_ar(rho,sigmax,sigmay,n,d):\n",
    "    X       = np.zeros(shape=(n,d))\n",
    "    Y       = np.zeros(n)\n",
    "    epsilon = np.random.normal(loc=0,scale=1,size = X.shape)\n",
    "    eta     = np.random.normal(loc=0,scale=1,size = n)\n",
    "    Y[0]    = np.sum(np.cos(X[0,:])) + sigmax*eta[0]\n",
    "    for k in range(1,n):\n",
    "        X[k,:] = (rho*X[k-1,:] + sigmax*epsilon[k,:])%(2*np.pi)\n",
    "        Y[k]   = np.sum(np.cos(X[k,:])) + sigmay*eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n = 2000\n",
    "d = 4\n",
    "\n",
    "rho    = 0.1\n",
    "sigmax = 0.1 \n",
    "sigmay = 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices X and Y containing data simulated according to the observation model described above may now be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data  \n",
    "X, Y = sample_data_ar(rho,sigmax,sigmay,n,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate different algorithms, the dataset is decomposed into training and test data.\n",
    "In this type of time series analysis, the training data are the first values of $Y_k$ and the test are the last one. This corresponds to situations where one wants to predict future values of $Y$ given historical data.\n",
    "This is somehow different form the i.i.d. case where taining and test data are chosen randomly using the function ``sklearn.model_selection.train_test_split``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split variables and observations, using 90% of the data set to estimate f \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data = X, columns = ['X1', 'X2', 'X3', 'X4'])\n",
    "df['Y'] = Y\n",
    "\n",
    "nb_data_train = int(0.95*n)\n",
    "nb_diff       = n-nb_data_train\n",
    "df.head()\n",
    "X_train = df.iloc[0:nb_data_train,:-1] \n",
    "X_test  = df.iloc[-nb_diff:,0:-1]\n",
    "X_train.head()\n",
    "Y_train = df.iloc[0:nb_data_train,-1] \n",
    "Y_test  = df.iloc[-nb_diff:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first random forest prediction can be performed. \n",
    "``rf`` is the random forest function of sklearn when the number of trees in the forest is set to ``n\\_trees``.\n",
    "The forest is estimated using the training data ``X\\_train`` and ``Y\\_train`` and the values of $Y$ in the test set are compared to those predicted by the algorithm.\n",
    "The mean-squared error between these quantities is also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in the forest for an elementary random forest estimate\n",
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf,linestyle = \"dashed\", color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='red', linestyle = 'dashed', alpha = 0.5)\n",
    "plt.xlabel('Test observations', fontsize=12)\n",
    "plt.ylabel('Predicted observations', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions seem reasonable but it may be that these predictions are very accurate at first and then that the errors accumulate over time.\n",
    "To test if this is the case, plot the values of $Y$ in the test set (true and predicted) as functions of the time from the last training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve this first prediction, the different parameters of the random forest algorithm may be tuned. For example, an increase of the number of trees in the forest reduced the varaince of the Monte Carlo estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in the forest\n",
    "n_trees = 500\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf,linestyle = \"dashed\", color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='red', linestyle = 'dashed', alpha = 0.5)\n",
    "plt.xlabel('Test observations', fontsize=12)\n",
    "plt.ylabel('Predicted observations', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this increases substancially the computational time although predictions seem equally relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is now to improve the algorithm using a cross-validation scheme to estimate the best value of these paramrters. It is useful to look at the parameters used by default in the algorithm here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "Set a grid of parameters that will be tested by the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with RandomizedSearchCVCreate, a grid of tuning parameters is built\n",
    "# then a random search will be performed to test which parameter values yield the best \n",
    "# random forest estimate (see below).\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in the forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# max_features is the number of dimension considered to select the best split (the dimension along which a cell is cut)\n",
    "max_features = ['log2', 'sqrt']\n",
    "# The maximum depth of the tree corresponds to the maximum number of levels of the tree. \n",
    "# If not given, splits are performed until all cells contain less than min_samples_split samples.\n",
    "max_depth = np.arange(5,100,5)\n",
    "# min_samples_split is the minimum number of samples in a cell to allow a split.\n",
    "min_samples_split = [2, 3, 4, 5, 6, 7, 8]\n",
    "# A split is considered in the tree if it leaves at least min_samples_leaf training samples in each \n",
    "# subcell obtained after the spliting process.\n",
    "min_samples_leaf = [1, 2, 3, 4]\n",
    "# if bootstrap is true all the training dataset is used to build each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring all parameters combinations in this grid would be computationally prohibitive. \n",
    "An efficient alternative is to compare parameters chosen at random in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "# Random search among  all parameters (the number of possible combination is given. It is not a complete grid search !!!). \n",
    "# search across n_iter = 10 different combinations with a default 3-fold cross validation.\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=10, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the best explored random forest\n",
    "best_random = rf_random.best_estimator_\n",
    "y_pred_rf = best_random.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "plt.title('Dimension of the state space: %d, MSE %f' %(d,mse))\n",
    "plt.scatter(Y_test,y_pred_rf,linestyle = \"dashed\", color='b', s = 10, alpha = 0.8)\n",
    "plt.plot(Y_test,Y_test,color='red', linestyle = 'dashed', alpha = 0.5)\n",
    "plt.xlabel('Test observations', fontsize=12)\n",
    "plt.ylabel('Predicted observations', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf_random.predict(X_test)\n",
    "mse       = mean_squared_error(Y_test,y_pred_rf)\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkorange> 3. Random forests for real data - Brazilian inflation prediction </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, random forests are used to predic the Brazilian inflation based on\n",
    "# many observed variables, see https://github.com/gabrielrvsc/HDeconometrics/\n",
    "df = pd.read_csv('./BRinf')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of observations, number of variables\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``pandas.DataFrame.corr`` may be used to compute the pairwise correlations between columns (variables and inflation). These correlations can be displayed using ``sns.heatmap`` to highlight highly correlated variables (and those likely to have an impact on the inflation).\n",
    "\n",
    "See ``feature_importances_`` below for a first try at (relevant) variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))  \n",
    "sns.heatmap(corr, xticklabels = False, yticklabels = False, cmap = 'Blues', ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_data_train = 140\n",
    "nb_diff       = df.shape[0]-nb_data_train\n",
    "# inflation observations\n",
    "Y_train = df.iloc[0:nb_data_train,1] \n",
    "Y_test  = df.iloc[-nb_diff:,1] \n",
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other variables\n",
    "X_train = df.iloc[0:nb_data_train,2:] \n",
    "X_test  = df.iloc[-nb_diff:,2:] \n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many variables are used for the inflation prediction while very few observations are available.\n",
    "# Selecting the most valuable variables is an alternativ to learn a simpler models\n",
    "# This is obtained in Python with rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,12))\n",
    "plt.bar(list(df)[2:93],rf.feature_importances_,align='center')\n",
    "plt.xticks(range(len(list(df)[2:93])),list(df)[2:93],rotation=90,size='small')\n",
    "plt.title('Feature importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier and RandomForestRegressor use the gini importance mechanism as a measure of the \n",
    "# fetaures importance. The mean decrease in impurity importance of a \n",
    "# feature is computed by measuring t%%javascript\n",
    "# see https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect the indices of the features with highest importance.\n",
    "nb_features_to_keep = 10\n",
    "ind = rf.feature_importances_.argsort()[-nb_features_to_keep:]\n",
    "X_train.iloc[:,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train.iloc[:,ind],Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test.iloc[:,ind])\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./PCA_cor.png\" width=\"500\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MSE as a function of the numbers of variables used to estimate the function f\n",
    "d_max = 60\n",
    "MSE   = []\n",
    "rf    = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train,Y_train)\n",
    "for d in range(2,d_max): \n",
    "    rfd = RandomForestRegressor(n_estimators = n_trees)\n",
    "    ind = rf.feature_importances_.argsort()[-d:]\n",
    "    rfd.fit(X_train.iloc[:,ind],Y_train)\n",
    "    # compute predictions usting test data and associated mean square error\n",
    "    y_pred_rf = rfd.predict(X_test.iloc[:,ind])\n",
    "    MSE = np.append(MSE,mean_squared_error(Y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(1,figsize=(8,8))\n",
    "plt.plot(MSE,marker='o',color='k')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Mean square error on the test data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the indices of the features with highest importance.\n",
    "nb_features_to_keep = 4\n",
    "ind = rf.feature_importances_.argsort()[-nb_features_to_keep:]\n",
    "X_train.iloc[:,ind]\n",
    "n_trees = 100\n",
    "rf      = RandomForestRegressor(n_estimators = n_trees)\n",
    "rf.fit(X_train.iloc[:,ind],Y_train)\n",
    "\n",
    "# compute predictions usting test data and associated mean square error\n",
    "y_pred_rf = rf.predict(X_test.iloc[:,ind])\n",
    "\n",
    "plt.figure(1,figsize=(8,8))\n",
    "plt.plot(Y_test.values,marker='o',color='k')\n",
    "plt.plot(y_pred_rf,'g--',marker='+')\n",
    "plt.legend(labels=('Future inflation values','Random forest estimates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may add here a cross validation procedure using RandomizedSearchCV or GridSearchCV\n",
    "# to select the best parameters (and compare the mean square error with what you obtain above)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
