{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkorange> Logistic regression & Feed forward neural networks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "np.set_printoptions(precision=2) \n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import check_grad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred>Logistic regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightharpoondown$ The objective is to predict the  label $Y\\in\\{0,1\\}$ based on $X\\in\\mathbb{R}^d$.\n",
    "\n",
    "$\\rightharpoondown$ Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "where $w \\in \\mathbb{R}^d$ is a vector of model weights and $b \\in \\mathbb{R}$ is the intercept, and where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\sigma: z \\mapsto \\frac{1}{1 + e^{-z}}\\,.\n",
    "$$\n",
    "\n",
    "$\\rightharpoondown$ The sigmoid function is a \\alert{model choice to map $\\mathbb{R}$ into $(0,1)$}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    expx = np.exp(x)\n",
    "    z   = expx / (1. + expx)\n",
    "    return z\n",
    "\n",
    "def sample_logistic(w0, n_samples=1000, corr=0.5):\n",
    "    n_features = w0.shape[0]\n",
    "    cov        = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X          = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p          = sigmoid(X.dot(w0))\n",
    "    y          = np.random.binomial(1, p, size=n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Prove that the Bayes classifier is linear in the logistic regression setting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(x,z,linestyle = \"dashed\",color=\"blue\", label= \"Sigmoid function\")\n",
    "plt.plot(x,.5*np.ones(np.size(x)),linestyle = \"dashed\",color=\"red\", label=\"Threshold 1/2\")\n",
    "plt.ylabel('Probability of  the event {Y=1}', fontsize=12)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "The graph above illustrates that the Bayes classification rule in this case is\n",
    "$f^*(X) = 1$ if and only if $\\langle w,X \\rangle + b>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples  = 1000\n",
    "n_features = 2\n",
    "\n",
    "w0   = multivariate_normal([-2,1], np.eye(2))\n",
    "\n",
    "X, y = sample_logistic(w0, n_samples=n_samples, corr = 0.3)\n",
    "\n",
    "simulated_data          = pd.DataFrame(columns = [\"x\",\"y\",\"Label\"])\n",
    "simulated_data[\"x1\"]     = X[:,0]\n",
    "simulated_data[\"x2\"]     = X[:,1]\n",
    "simulated_data[\"Label\"] = y\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X,w):\n",
    "    z = sigmoid(X.dot(w))\n",
    "    return z\n",
    "    \n",
    "xlim  = [np.min(X[:,0]), np.max(X[:,0])]\n",
    "ylim  = [np.min(X[:,1]), np.max(X[:,1])]\n",
    "xplot = np.linspace(xlim[0], xlim[1], 30)\n",
    "yplot = np.linspace(ylim[0], ylim[1], 30)\n",
    "\n",
    "Yplot, Xplot = np.meshgrid(yplot, xplot)\n",
    "xy           = np.vstack([Xplot.ravel(), Yplot.ravel()]).T\n",
    "P            = decision_function(xy,w0).reshape(Xplot.shape)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");\n",
    "\n",
    "# plot decision boundary and margins\n",
    "plt.contour(Xplot, Yplot, P, colors = 'k', levels = [0.5], alpha = 0.8, linestyles = ['--']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Logistic regression: losses and gradients </font>\n",
    "\n",
    "The aim of this section is to detail how to solve the following optimization problem\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\\,,\n",
    "$$\n",
    "where $d$ is the number of features.\n",
    "\n",
    "$$\n",
    "f: w \\mapsto \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\{-y_ix_i'w + \\log(1 + \\exp(x_i' w))\\} + \\frac{\\lambda}{2} \\|w\\|_2^2\\,,\n",
    "$$\n",
    "where $n$ is the sample size, and where $y_i \\in \\{ 0, 1 \\}$ for all $1\\leqslant i\\leqslant n$.\n",
    " \n",
    "A basic gradient descent algorithm requires to compute the functions $f$ and $\\nabla f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Write the likelihood of the logistic regression model and its gradient (blackboard)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, X, y, lmbd):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lmbd = lmbd\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        # Computes f(w)\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        for k in range(n_samples):\n",
    "            res += np.log(1+np.exp(X[k].dot(w))) - y[k]*X[k].dot(w)\n",
    "        res = res/n_samples\n",
    "        res += lmbd*0.5*(norm(w)**2)\n",
    "        return res\n",
    "    \n",
    "    def grad(self, w):\n",
    "        # Computes the gradient of f at w\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        for k in range(n_samples):\n",
    "            res += -y[k]*X[k] + np.exp(X[k].dot(w))/(1+np.exp(X[k].dot(w)))*X[k]\n",
    "        res = res/n_samples\n",
    "        res += lmbd*w\n",
    "        return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numerically the gradient using the function checkgrad from scipy.optimize\n",
    "# Use the function simu_logreg to simulate data according to the logistic regression model\n",
    "n_features = 10\n",
    "w_true     = np.random.randn(n_features)\n",
    "X, y       = sample_logistic(w_true, n_samples, corr=0.1)\n",
    "model      = LogisticRegression(X, y, 1e-3)\n",
    "# check_grad assesses the correctness of a gradient by comparing it to a finite-difference approximation\n",
    "check_grad(model.loss, model.grad, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Gradient descent </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of machine/deep learning applications, the function to be minimized is of the form:\n",
    "$$\n",
    "f:w\\mapsto  \\frac{1}{n}\\sum_{i=1}^n \\ell(Y_i, \\langle w; X_i \\rangle) + \\lambda g(w) = \\frac{1}{n}\\sum_{i=1}^n f_i(w)\\,.\n",
    "$$\n",
    "The most simple method  is based on full gradients, since at each iteration  it requires to compute\n",
    "$$\n",
    "\\nabla f(w) = \\frac 1n \\sum_{i=1}^n \\nabla  f_i(w)\\,,\n",
    "$$\n",
    "which depends on the whole dataset. When processing very large datasets ($n$ is large), this approach has a highly prohibitive computational cost  for a  unique step towards the minimum. \n",
    "For all $k\\geqslant 1$, set\n",
    "$$\n",
    "w^{(k)} = w^{(k-1)} - \\eta_k \\nabla f_{}(w^{(k-1)})\\,.\n",
    "$$\n",
    "Each iteration has complexity $O(nd)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(model, w0, n_iter,step):\n",
    "    loss_val = np.zeros(n_iter+1)\n",
    "    w        = w0.copy()\n",
    "    w_new    = w0.copy()\n",
    "    for k in range(n_iter + 1):\n",
    "        w_new[:]    = w-step*model.grad(w)\n",
    "        w[:]        = w_new\n",
    "        loss_val[k] = model.loss(w)\n",
    "    return w, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sgd   = 1e-1\n",
    "w0         = np.random.randn(n_features)\n",
    "n_iter_max = 500\n",
    "\n",
    "w, gd_loss  = gd(model, w0, n_iter = n_iter_max, step = step_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (6, 5))\n",
    "plt.plot(gd_loss, '-')\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Negative loglikelihood')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Roc curve and auc</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = int(0.2*n_samples)\n",
    "X_test, y_test = sample_logistic(w_true, n_samples = n_test, corr = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted class using the estimated model\n",
    "y_pred = y_test.copy()\n",
    "for i in range(n_test):\n",
    "    if X_test[i].dot(w)>0:\n",
    "        y_pred[i] = 1.0\n",
    "    else:\n",
    "        y_pred[i] = 0.0\n",
    "print('Mean prediction error with the estimated parameter: %f'%np.mean(np.abs(y_pred-y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probability for each new individual using the estimated model\n",
    "y_score = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "    y_score[i] = sigmoid(X_test[i].dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve assesses the diagnostic ability of the classifier  as the classification threshold is modified. Logistic regression models the distribution of $Y$ given $X$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "and the Bayes classifier is defined as \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) > \\mathbb{P}(Y = 0| X)$,\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\langle w,X \\rangle + b>0$,\n",
    "\n",
    "or also to \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) >1/2$. \n",
    "\n",
    "Therefore, the theoretical threshold to classify individuals is $1/2$. However, analyzing the sensitivity of the classifier to this threshold may be interesting which is the aim of the ROC curve which displays the True positive rate as a function of the False positive rate when the threshold is changed. For each value $p^*\\in(0,1)$ the ROC curve classifies individuals using\n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) > p^*$\n",
    "\n",
    "and plots the True positive rate as a function of the False positive rate. \n",
    "\n",
    "Depending on the application, an optimal threshold may then be used to obtain satisfying True and False positive rates on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score, pos_label=1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(fpr,tpr,linestyle = \"dashed\",color=\"blue\")\n",
    "plt.title('ROC curve')\n",
    "plt.ylabel('True positive rate', fontsize=12)\n",
    "plt.xlabel('False positive rate', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The area under the ROC curve (AUC) is: %f'%metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Softmax regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be extended to classify data in more than two groups. Softmax regression provides a model for the probability that an input $x$ is associated with each group.  It is assumed that the probability to belong to the class $k\\in\\{1,\\ldots,M\\}$ can be expressed by \n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = k| X) = \\frac{\\exp(\\langle w_k,X \\rangle + b_k)}{\\sum_{\\ell=1}^{M}\\exp(\\langle w_\\ell,X \\rangle + b_\\ell)} = p_k(X)\\,,\n",
    "\\end{equation*}\n",
    "where $w_\\ell \\in \\mathbb{R}^d$ and $b_\\ell$  are model \\textbf{weights} and \\textbf{intercepts} for each class.\n",
    "\n",
    "\n",
    "To estimate these unknown parameters, a maximum likelihood approach is used as in the logistic regression setting. In this case, the loss function is given by the negative log-likelihood (see also the section on gradient based method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Write the likelihood of the softmax regression model and its gradient (blackboard)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Handwritten digit recognition with MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The MNIST (http://yann.lecun.com/exdb/mnist) dataset contains images representing handwritten digits.  Each input image is a 28 x 28 matrix, each entry representing a gray level.  Each input data is then a $d=784$ dimensional vector.  \n",
    "\n",
    "The labels in $\\{0, 1, 2, \\ldots, 9\\}$ can represented using one-hot encoding: labels in $\\{0, 1, 2, \\ldots, 9\\}$ are replaced by labels in $\\{ 0, 1\\}^{10}$, namely $0$ is replaced by $(1, 0, \\ldots 0)$, $1$ is replaced by $(0, 1, 0, \\ldots 0)$, $2$ is replaced by $(0, 0, 1, 0, \\ldots, 0)$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import activations\n",
    "#from keras import backend as K\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train     = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test      = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# normalize the input data\n",
    "x_train = x_train/255\n",
    "x_test  = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "for i in range(6):\n",
    "    plt.subplot(1, 6, i+1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "    plt.title('Label = %d' % y_train[i], fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A softmax regression model can be designed and estimated using Keras as this model is a feed froward neural network with only one layer. Each input data $X$ is represented by a 784 dimensional vector and the first layer is made of $10$ neurons each one corresponding to one class and computing $p_k(X)$ for each $0\\leqslant k\\leqslant 9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model prone to add layers sequentially\n",
    "model = Sequential()\n",
    "# flatten the data replaces 28 * 28 matrices by a 784 dimensional vector\n",
    "# This is always necessary before a fully-connected layer (Dense object)\n",
    "model.add(Flatten(input_shape=input_shape, name='flatten'))\n",
    "# add one dense (fully connected layer) with softmax activation function\n",
    "# As it is the first layer, the input size is mandatory\n",
    "model.add(Dense(num_classes, activation='softmax', name='dense_softmax'))\n",
    "\n",
    "# \"compile\" this model, \n",
    "model.compile(\n",
    "    # specify the loss as the cross-entropy\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    # choose the gradient based method to estimate the parameters\n",
    "    # see https://keras.io/optimizers/ to have an overview of the different options\n",
    "    # see also section 2 on gradient based methods.\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    # metric to monitor on the test data\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['acc'], lw=1, label='Train')\n",
    "plt.plot(history.epoch, history.history['val_acc'], lw=1, label='Test')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of softmax regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.get_layer('dense_softmax').get_weights()\n",
    "imgs = weights.reshape(28, 28, 10)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "vmin, vmax = imgs.min(), imgs.max()\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    im = imgs[:, :, i]\n",
    "    mappable = ax.imshow(im, interpolation=\"nearest\", \n",
    "                         vmin=vmin, vmax=vmax, cmap='bwr')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"%i\" % i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Feed-Forward Neural Network (FFNN) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax regression of the previous section is a linear model, with 7850 parameters.  It is easy to fit, numerically stable, but might be too simple for some tasks.  The idea underlying neural networks is to have successive \"neurons\" performing a linear transformation of the input data (depending on a weight matrix and a bia vector) followed by an activation function to design more flexible models with additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for a fully connected feed-forward neural network with one hidden layer \n",
    "# with 128 units and a relu activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn = Sequential()\n",
    "\n",
    "model_ffnn.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_ffnn.add(Dense(128, activation='relu'))\n",
    "\n",
    "model_ffnn.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are involved in this model ?\n",
    "\n",
    "The input size is 28*28 = 724. \n",
    "\n",
    "This input is transformed linearly in 128 hidden units in the dense layer which lead to 128*784 + 128 = 100480 parameters to obtain the 128 units.\n",
    "\n",
    "These units are transformed in a ``softmax`` activation function which adds 10*128 + 10 = 1290 parameters.\n",
    "\n",
    "This Feed Forward Neural Networks depends on 101770 parameters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffnn.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_ffnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 25\n",
    "history = model_ffnn.fit(x_train, y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         validation_data=(x_test, y_test))\n",
    "score = model_ffnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['acc'], lw=3, label='Training')\n",
    "plt.plot(history.epoch, history.history['val_acc'], lw=3, label='Testing')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of softmax regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
