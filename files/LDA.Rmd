---
title: "Linear Discriminant Analysis"
author: ""
date: "2018-2019"
output:
  html_document: default
  pdf_document: default
---

```{r global_Options, echo=TRUE, include = FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, include = TRUE, eval=FALSE)
```


```{r, include=FALSE}
#Pour convertir le Rmarkdown en un fichier R, on peut utiliser la commande suivante dans la console
#knitr::purl("01_Logistic.Rmd")
```



```{r, echo=TRUE, include=TRUE, eval = TRUE}
library(MASS)
library(matlib) 
library(ggplot2)
```

This lab sets the focus on Linear Discriminant Analysis (LDA), a two-class classification algorithm using the library $\texttt{MASS}$. 

Let $(X,Y)$ be a couple of random variables with values in $\mathbb{R}^2 \times \{0,1\}$ and satisfying, for all $k\in\{0,1\}$ and all $A\in \mathcal{B}(\mathbb{R}^2)$,
\begin{equation}
\label{eq:lda:mixture}
\mathbb{P}(Y=k)=\pi_{k}>0 \quad\textrm{and}\quad \mathbb{P}(X\in A|Y=k)=
\int_A g_{k}(x)\,\mathrm{d}x,
\end{equation}
where $\pi_{0}+\pi_{1}=1$ and $g_0, g_1$ are two probability densities in
$\mathbb{R}^2$. In this lab, it is assumed that the conditional distribution of $X$ given $Y$ is Gaussian, i.e., for $k\in\{1,2\}$,
$$
g_{k}: x \mapsto \det(2\pi\Sigma_k)^{-1/2}\exp\left(-{1\over 2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right)\,,
$$
with $\Sigma_0$, $\Sigma_1$ symmetric definite positive matrices and $\mu_0,\mu_1\in \mathbb{R}^2$,  $\mu_0\ne\mu_1$.

**1. Assume that $\pi_0 = \pi_1 = 0.5$, $\Sigma_0 = \Sigma_1 = I_2$, $\mu_1 = (0,1)$ and $\mu_2 = (5,10)$. Simulate $n=1000$ independent couples $(X_i, Y_i)$ distributed as $(X,Y)$. Plot these data, by assigning different colors to data labelled as $0$ or $1$.** 

```{r, echo =TRUE, include=TRUE, eval=TRUE}
# Data generation 
n       <- 1000
mu_0    <- c(0,1)
mu_1    <- c(5,10)
pi_0    <- 0.5
Sigma_0 <- diag(2)
Sigma_1 <- diag(2)

draw_rv = function(j, pi_0){
  y = rbinom(1,1,prob=1-pi_0)
  if (y==0) {
    return(c(mvrnorm(1,mu_0, Sigma_0),0))
  }
  else {return(c(mvrnorm(1,mu_1, Sigma_1),1))}
}
dataset           = data.frame(t(sapply(c(1:n), FUN = draw_rv, pi_0 = 0.5)))
colnames(dataset) = c("X1", "X2", "Class")
dataset$output    <- as.factor(dataset$output)
dataset           = dataset[sample(1:n),]

ggplot(dataset,aes(x = X1,y = X2, color = output)) + geom_point() + theme_minimal()



#plot(dataset$X1, dataset$X2, col = dataset$output, xlab="", ylab="", cex = 0.4)

## Other coding
# don = matrix(NA, n, 3)
# colnames(don) = c("y", "X1", "X2")
# don[,1] = rbinom(n, 1, prob = 1-pi_0)
# ind = (don[,1]==0)
# don[ind,2:3] = mvrnorm(length(which(ind)),mu_0, Sigma_0)
# don[!ind,2:3] = mvrnorm(length(which(!ind)),mu_1, Sigma_1)
# plot(X2~X1, col = as.factor(y), data = don, cex = 0.4)
```

**2. What is the distribution of $X$? **

We have 
		\begin{align*}
		\mathbb{P} [X \in \textrm{d} x ] & = \mathbb{P} [Y = 0] \mathbb{P}[X \in \textrm{d}x | Y=0] + \mathbb{P} [Y = 1] \mathbb{P}[X \in \textrm{d}x | Y=1] \\
		& = (\pi_0 g_0 (x) + \pi_1 g_1(x)) \textrm{d}x.
		\end{align*}

**3. We define the classifier $h^{\star}:\mathbb{R}^2\to\{0,1\}$ by, for all  $x\in\mathbb{R}^2$, **
 $$h^{\star}(x)={\bf 1}_{\{\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)\}} \, .$$
**Prove that the classifier $h^{\star}$ fulfills**
	$$\mathbb{P}(h^{\star}(X)\neq Y)=\min_{h}\mathbb{P}(h(X)\neq Y),$$
**where the minimum is taken over all classifier $h :\mathbb{R}^2\to\{0,1\}$.**

We know that the classifier $h^{\star}$, which minimizes the risk $\mathbb{P} [h(X) \neq Y]$ among all classifiers, is the Bayes classifier defined as
		\begin{align*}
		h^{\star}(x) = \left\lbrace
		\begin{array}{ll}
		0 & \textrm{if}~\eta(x) \leq 1/2\\
		1 & \textrm{if}~\eta(x) > 1/2,
		\end{array}
		\right.
		\end{align*}
		where $\eta(x) = \mathbb{P}[Y=1|X \in \textrm{d}x]$. In our particular case, we have 
		\begin{align*}
		\mathbb{P}[Y=1|X \in \textrm{d}x] = \frac{\mathbb{P} [X \in \textrm{d} x | Y=1] \mathbb{P}[Y=1]}{\mathbb{P}[X \in \textrm{d}x] } 
		=  \frac{\pi_1 g_1(x)}{\pi_0 g_0(x) + \pi_1 g_1(x)},
		\end{align*}
		and the condition $\eta(x) \leq 1/2$ can be rewritten as
		\begin{align*}
		& \frac{\pi_1 g_1(x)}{\pi_0 g_0(x) + \pi_1 g_1(x)} \leq 1/2,
		\end{align*}
		that is $\pi_1 g_1(x) \leq \pi_0 g_0(x)$,
		Consequently, $h^{\star}(x) = \mathbb{1}_{\pi_1 g_1(x) > \pi_0 g_0(x)} = h_*(x)$, which proves that $h_*$ is the Bayes classifier. 
		
**4. General case. In this question, we go back to the general setting and only assume that $\Sigma_0 = \Sigma_1 = \Sigma$ are non-singular, and that $\mu_0 \neq \mu_1$. With these assumptions, prove that $\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)$ is equivalent to**
	$$(\mu_{1}-\mu_{0})^T\Sigma^{-1}\left(x-{\mu_{1}+\mu_{0}\over
		2}\right)>\log(\pi_{0}/\pi_{1}).$$ **Interpret geometrically this result.**

We start from 
		\begin{align*}
		& \pi_1 g_1(x) > \pi_0 g_0(x) \\
		\Leftrightarrow & \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\\
		\Leftrightarrow & -{1\over 2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1)\\
		\Leftrightarrow		& -{1\over 2}\Big(  -\mu_1^T\Sigma^{-1}x + \mu_1^T\Sigma^{-1}\mu_1  - x^T\Sigma^{-1}\mu_1  +\mu_0^T\Sigma^{-1}x - \mu_0^T\Sigma^{-1}\mu_0 + x^T\Sigma^{-1}\mu_0             \Big)  > \log(\pi_0/ \pi_1)\\
		\Leftrightarrow		& x^T\Sigma^{-1}\mu_1 - x^T\Sigma^{-1}\mu_0 - \frac{1}{2} \mu_1^T\Sigma^{-1}\mu_1 + \frac{1}{2} \mu_0^T\Sigma^{-1}\mu_0  > \log(\pi_0/ \pi_1)\\
		\Leftrightarrow		& (\mu_1 - \mu_0)^T \Sigma^{-1} \Big(x - \frac{\mu_1 + \mu_0}{2}\Big)  > \log(\pi_0/ \pi_1).
		\end{align*}
		
		
5. Apply this result to the data simulated in question 1 and plot the boundary decision of classifier $h^{\star}$, i.e. the line which separates $\{x \in \mathbb{R}^2, h^{\star}(x) = 0 \}$ from $\{x \in \mathbb{R}^2, h^{\star}(x) = 1 \}$.


```{r, echo = TRUE, eval = TRUE, include = TRUE, cache = TRUE}
boundary_true_parameters = function(x, mu0, mu1, Sigma, pi0){
  u = t(as.matrix(mu1-mu0)) %*% inv(Sigma)
  v = (u %*% (matrix(x - ((mu1+mu0)/2)) )) - log(pi0/(1-pi0))
  return(as.numeric(v)) 
}
list_x1<-seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2<-seq(min(dataset$X2), max(dataset$X2),length=100)
u = expand.grid(list_x1, list_x2)
res = apply(u, MARGIN = 1, FUN = boundary_true_parameters, mu0=mu_0, mu1=mu_1, Sigma= Sigma_1, pi0 = pi_0)
res = matrix(res, nrow = 100, ncol =100)

contour(list_x1,list_x2,res,levels=0)
points(dataset$X1, dataset$X2, col = dataset$output, cex = 0.4)
```

6. In real life, one does not know in advance the different parameters ($\Sigma_0, \Sigma_1, \mu_0, \mu_1, \pi_0, \pi_1$) but fortunately, we often have data at our disposal. Propose a way to use these data to estimate the previous underlying parameters and thus implement question 5 in the case of unknown parameters. Compare the classifiers of question 5 and 6. Comment.

```{r, echo = TRUE, eval = TRUE, include = TRUE}
est_pi_0 = length(which(dataset$output==0))/dim(dataset)[1]
est_pi_1 = 1 - est_pi_0

est_mu_0 = colMeans(dataset[which(dataset$output==0),c(1,2)])
est_mu_1 = colMeans(dataset[which(dataset$output==1),c(1,2)])

est_Sigma_0 = cov(dataset[which(dataset$output==0),c(1,2)], dataset[which(dataset$output==0),c(1,2)])

est_Sigma_1 = cov(dataset[which(dataset$output==1),c(1,2)], dataset[which(dataset$output==1),c(1,2)])

list_x1<-seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2<-seq(min(dataset$X2), max(dataset$X2),length=100)
u = expand.grid(list_x1, list_x2)
est_res = apply(u, MARGIN = 1, FUN = boundary_true_parameters, mu0=est_mu_0, mu1=est_mu_1, Sigma= est_Sigma_0, pi0 = est_pi_0)
est_res = matrix(res, nrow = 100, ncol =100)

contour(list_x1,list_x2,est_res,levels=0, lwd= 5, col= 5)

points(dataset$X1, dataset$X2, col = dataset$output)

contour(list_x1,list_x2,res,levels=0)

points(dataset$X1, dataset$X2, col = dataset$output)


```

7. Let us now generate new data as in question 1 but with $\Sigma_0 \neq I_2$. We can for example randomly choose a matrix among a large set of covariance matrix. Reminder: a covariance matrix must be symmetric positive-definite. 

```{r, echo = TRUE, eval = TRUE, include = TRUE, cache = TRUE}
#Data generation 
n = 1000
mu_0 = c(0,1)
mu_1 = c(5,10)
pi_0 = 0.5
Sigma_0 = matrix(c(1,0,0,1), nrow = 2, ncol = 2)

A <- matrix(runif(4), ncol=2) 
Sigma_1 <- t(A) %*% A
det(Sigma_1)

draw_rv = function(j, pi_0){
  y = rbinom(1,1,prob=1-pi_0)
  if (y==0) {
    return(c(mvrnorm(1,mu_0, Sigma_0),0))
  }
  else {return(c(mvrnorm(1,mu_1, Sigma_1),1))}
}


dataset = as.data.frame(t(sapply(c(1:n), FUN = draw_rv, pi_0 = 0.5)))
colnames(dataset) = c("X1", "X2", "output")
dataset$output <- as.factor(dataset$output)
dataset = dataset[sample(1:n),]
plot(dataset$X1, dataset$X2, col = dataset$output, xlab="", ylab="")
```


8. Since the covariance matrices are distinct, the boundary found in question 4 is not valid anymore. Prove that $\pi_{1}g_{1}(x)>\pi_{0}g_{0}(x)$ is equivalent to
	
	\begin{align*}
	x^T\tilde{\Sigma} x + u^Tx + c >0,
	\end{align*}
	where
	\begin{align*}
	\tilde{\Sigma} & = \frac{\Sigma_0^{-1} - \Sigma_1^{-1}}{2}, \\
	u & = \Sigma_1^{-1} \mu_1 - \Sigma_0^{-1} \mu_0\\
	c & = \frac{1}{2}\big( \mu_0^T\Sigma_0^{-1} \mu_0 - \mu_1^T\Sigma_1^{-1} \mu_1) - \log \Big(\frac{\pi_0}{1 - \pi_0}\Big) - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ).
	\end{align*}
	
If we do not assume anymore that $\Sigma_0 = \Sigma_1$, then the decision boundary is given by a quadric: the term $x^T\Sigma_1x - x^T\Sigma_0x$ in question $3$ does not cancel anymore. More precisely, \begin{align*}
		& \pi_1 g_1(x) > \pi_0 g_0(x) \\
		\Leftrightarrow & \log(\pi_1 g_1(x)) > \log(\pi_0 g_0(x))\\
		\Leftrightarrow & -{1\over 2}(x-\mu_1)^T\Sigma_1^{-1}(x-\mu_1) + {1\over 2}(x-\mu_0)^T\Sigma_0^{-1}(x-\mu_0) > \log(\pi_0/ \pi_1) + 0.5\log(| \det(\Sigma_1) | ) - 0.5\log(| \det(\Sigma_2) | )\\
		\Leftrightarrow		& -{1\over 2}\Big(x^T \Sigma_1^{-1} x - x^T \Sigma_0^{-1} x   -\mu_1^T\Sigma^{-1}x + \mu_1^T\Sigma^{-1}\mu_1  - x^T\Sigma^{-1}\mu_1  +\mu_0^T\Sigma^{-1}x  - \mu_0^T\Sigma^{-1}\mu_0 + x^T\Sigma^{-1}\mu_0             \Big)
		- \log(\pi_0/ \pi_1) \\
		& \qquad - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ) >0\\
		\Leftrightarrow		& \frac{1}{2} x^T \Big( \Sigma_0^{-1} - \Sigma_1^{-1} \Big) x + \Big( \mu_1^T \Sigma_1^{-1} - \mu_0^T \Sigma_0^{-1} \Big) x  + \frac{1}{2} \Big( \mu_0^T \Sigma_0^{-1} \mu_0 - \frac{1}{2} \mu_1^T \Sigma_1^{-1} \mu_1\Big) - \log\Big(\frac{\pi_0}{1 - \pi_0}\Big)  \\
		& \qquad - 0.5\log(| \det(\Sigma_1) | ) + 0.5\log(| \det(\Sigma_2) | ) >0,
		\end{align*} 
		which gives the desired result. 
		
		

9. As in question 5, plot the previous boundary decision with the true parameter values. 

```{r, echo = TRUE, include= TRUE, eval = TRUE}
boundary_true_parameters_quadratic = function(x, mu0, mu1, Sigma0, Sigma1, pi0){

  u1 =  -0.5*(t(as.matrix(x-mu1))) %*% inv(Sigma1) %*% as.matrix((x - mu1))
  u0 =  0.5*(t(as.matrix(x-mu0))) %*% inv(Sigma0) %*% as.matrix((x - mu0))
  cste = - log(pi0/(1-pi0))
  bonus = -0.5*log(abs(det(Sigma1))) + 0.5*log(abs(det(Sigma0)))
  
  return(as.numeric(u1+u0+cste+bonus)) 
}
list_x1<-seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2<-seq(min(dataset$X2), max(dataset$X2),length=100)
u = expand.grid(list_x1, list_x2)
res = apply(u, MARGIN = 1, FUN = boundary_true_parameters_quadratic, mu0=mu_0, mu1=mu_1, Sigma0 = Sigma_0, Sigma1 = Sigma_1, pi0 = pi_0)
res_quad_true = matrix(res, nrow = 100, ncol =100)
contour(list_x1,list_x2,res_quad_true,levels=0)
points(dataset$X1, dataset$X2, col = dataset$output, cex = 0.4)
```


10. In real-world application, one does not know the "true" parameters. Apply the previous question by estimating the unknown parameters of the model. 

```{r, echo = TRUE, eval = TRUE, include = TRUE}
est_pi_0 = length(which(dataset$output==0))/dim(dataset)[1]
est_pi_1 = 1 - est_pi_0

est_mu_0 = colMeans(dataset[which(dataset$output==0),c(1,2)])
est_mu_1 = colMeans(dataset[which(dataset$output==1),c(1,2)])

est_Sigma_0 = cov(dataset[which(dataset$output==0),c(1,2)], dataset[which(dataset$output==0),c(1,2)])

est_Sigma_1 = cov(dataset[which(dataset$output==1),c(1,2)], dataset[which(dataset$output==1),c(1,2)])

list_x1<-seq(min(dataset$X1),max(dataset$X1),length=100)
list_x2<-seq(min(dataset$X2), max(dataset$X2),length=100)
u = expand.grid(list_x1, list_x2)
res = apply(u, MARGIN = 1, FUN = boundary_true_parameters_quadratic, mu0=est_mu_0, mu1=est_mu_1, Sigma0 = est_Sigma_0, Sigma1 = est_Sigma_1, pi0 = est_pi_0)
res_quad_true = matrix(res, nrow = 100, ncol =100)

contour(list_x1,list_x2,res_quad_true,levels=0)
points(dataset$X1, dataset$X2, col = dataset$output, cex = 0.4)
```

